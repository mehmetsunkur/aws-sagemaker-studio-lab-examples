{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846a9ce0-6367-47d1-bc58-d3a2338628f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "[![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/mehmetsunkur/aws-sagemaker-studio-lab-examples/blob/main/generative-ai/mistral/prompting-mistral7B.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6602da5-3c8d-40dd-9211-a0009357d69a",
   "metadata": {},
   "source": [
    "# Experimenting Prompt Engineering - Chatbot\n",
    "\n",
    "In this notebook we will explore common prompt engineering techniques to get the most out of Large Language Models (LLMs). \n",
    "\n",
    "First you will experiment by loading a 7 billion parameter LLM within the notebook environment and throwing some prompts its way to see how differt prompt strategies can impact the results of the model. \n",
    "\n",
    "After trying different prompt engineering techniques, you will learn how to run a simple chatbot using what you learned. \n",
    "\n",
    "Finally, we'll review some pointers for how to put these concepts into practice by building on the sample code and swapping in different LLMs to create your own solutions.\n",
    "\n",
    "This notebook is used at workshop named AIM219 at re:Invent. Please refer the presentation slide from here: [Learn and experiment with LLMs in Amazon SageMaker Studio Lab (AIM219)](https://speakerdeck.com/icoxfog417/learn-and-experiment-with-llms-in-amazon-sagemaker-studio-lab-aim219). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091fdae7-bb30-4e60-a2ec-01eaf20657e2",
   "metadata": {},
   "source": [
    "### Working Environment \n",
    "\n",
    "[![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/aws/studio-lab-examples/blob/main/generative-ai/mistral/prompting-mistral7B.ipynb)\n",
    "\n",
    "\n",
    "This notebook has been designed, written and tested to run for free on [Amazon SageMaker Studio Lab](https://studiolab.sagemaker.aws/) with CPU.  Studio Lab is a free machine learning (ML) development environment that provides compute and storage (up to 15GB) at no cost with NO credit card required.\n",
    "\n",
    "You can sign up for Amazon SageMaker Studio Lab here: https://studiolab.sagemaker.aws/\n",
    "\n",
    "> Whatever environment you end up using, make sure you have at least 12 GB of disk space available to run this code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8a19a-8522-4bcf-b2a5-54cfcad0aead",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "First, if needed, install `ctransformers` - Python bindings implemented in C/C++ to accelerate the inference speed of models built on [`transformers` library](https://github.com/huggingface/transformers). If you run this notebook locally or on a GPU instance (ex. g4 or g5) instead of Studio Lab, please check that appropriate version of the CUDA runtime (`nvcc`) is installed. If you face a \"command not found\" error when running `nvcc`, [installing cuda-toolkit](https://anaconda.org/nvidia/cuda-toolkit) after checking which CUDA version is needed via `nvidia-smi` should fix the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfa1ac45-da8a-441e-83d6-65e8132bba2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (4.39.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers[torch]) (0.22.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers[torch]) (2023.12.25)\n",
      "Requirement already satisfied: filelock in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers[torch]) (3.13.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers[torch]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers[torch]) (0.4.2)\n",
      "Requirement already satisfied: requests in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers[torch]) (4.66.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers[torch]) (0.29.2)\n",
      "Requirement already satisfied: torch in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: psutil in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.3.1)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->transformers[torch]) (2.2.0)\n",
      "Requirement already satisfied: sympy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->transformers[torch]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->transformers[torch]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->transformers[torch]) (10.3.2.106)\n",
      "Requirement already satisfied: networkx in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->transformers[torch]) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->transformers[torch]) (11.0.2.54)\n",
      "Requirement already satisfied: jinja2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->transformers[torch]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->transformers[torch]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers[torch]) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers[torch]) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ctransformers in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.2.27)\n",
      "Requirement already satisfied: huggingface-hub in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ctransformers) (0.22.2)\n",
      "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ctransformers) (9.0.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (23.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (2024.3.1)\n",
      "Requirement already satisfied: filelock in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (3.13.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (6.0)\n",
      "Requirement already satisfied: requests in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (4.11.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->huggingface-hub->ctransformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->huggingface-hub->ctransformers) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->huggingface-hub->ctransformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->huggingface-hub->ctransformers) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.1.16)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (2.7.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (3.9.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (0.1.42)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (0.0.32)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (0.1.45)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers[torch]\n",
    "%pip install ctransformers\n",
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8dc104e-d8e9-489b-ab21-dbcecbc3ecca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce04e3ec-9978-45f5-9a00-e41ba50804f4",
   "metadata": {},
   "source": [
    "## Mistral-7B-Instruct-v0.1-GGUP Model\n",
    "\n",
    "The 🐋 [Mistral-7B-Instruct-v0.1-GGUP](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF) 🐋 model was fine-tuned on top of Mistral 7B using using a variety of publicly available conversation datasets. \n",
    "\n",
    "### Loading the model\n",
    "\n",
    "The following cell will load the pretrained model. Because the model itself is around 3.8 GB, so it can take a minute or so to load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d237a5b-0ee3-4b48-a426-4048b186a375",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e160c6d1225541cf8d0f9cb50a585d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbee50da30dd4772914461195cf7224a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "    model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "    model_type=\"mistral\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a31903-630d-4b12-bea0-a77a872b61ac",
   "metadata": {},
   "source": [
    "# How do large language models work?\n",
    "\n",
    "## Prompt engineering\n",
    "\n",
    "Remember that LLMs are trained to predict the next word when given a sequence of words. \n",
    "\n",
    "![LLM-concept](llm-generation.png)\n",
    "\n",
    "The way that we prompt an LLM can make a big difference in how it responds and how useful its output is. This depends not only on they type of query, but also nuances of how individual models where trained. Let's start by asking the Mistral model to describe the concept of photosynthesis.\n",
    "\n",
    "## Let's try a basic prompt\n",
    "\n",
    "In the following cell, we provide the model three parameters:\n",
    "1. the prompt: \"Explain photosythesis\"\n",
    "2. max_new_tokens which is the maximum length in tokens for the model's response\n",
    "3. temperature which you can think of as a measure of how *creative* the model can be in its response\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Note:</b> If you are running this on a cpu instance of Studio Lab keep in mind that the following cell may take a couple minutes to run.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d5a91d1-2198-4583-90b0-8841885b3e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight, water, and carbon dioxide into glucose (sugar), oxygen, and other chemical compounds. It is an essential process for the survival of most life forms on Earth, as it provides the primary source of energy and food for nearly all living organisms.\n",
      "\n",
      "Photosynthesis occurs in two stages: the light-dependent reactions and the light-independent reactions. During the light-dependent reactions, chlorophyll, a pigment found in plant cells, absorbs sunlight and converts it into chemical energy in the form of ATP (adenosine triphosphate) and NADPH (nicotinamide adenine dinucleotide phosphate). Water molecules are also split during this process, releasing oxygen as a byproduct.\n",
      "\n",
      "The light-independent reactions, also known as the Calvin cycle, use the ATP and NADPH generated in the light-dependent reactions to power the conversion of carbon dioxide into glucose. This process involves a series of chemical reactions that take place in the strom"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain photosythesis\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79335c8f-ede7-4a65-813e-ec225822d5db",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model did pretty well. Notice it started with generating a period (\".\"). That's because LLMs are trained to generate next most probably token, given an input. Let's see if we can make the answer more concise, and we will end our prompt with a period this time.</b></div>\n",
    "\n",
    "## Modifying the basic prompt\n",
    "Below we will try giving the model more specific instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18fe60fe-bc65-4855-819e-a7ecb3071e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Photosynthesis is the process by which plants, algae and some bacteria convert sunlight, water and carbon dioxide into glucose, oxygen and other chemical compounds. This process occurs in the chloroplasts of plant cells and involves two stages: the light-dependent reactions and the light-independent reactions. Through photosynthesis, plants produce their own food and release oxygen, which is essential for all living organisms."
     ]
    }
   ],
   "source": [
    "prompt = \"Explain photosythesis in three sentences.\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7071987-b179-4cd2-900c-28e02894c7d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>This response is more concise, our prompt worked quite well.</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc398e-65a6-4ed6-99bc-19a3f2fa0498",
   "metadata": {},
   "source": [
    "# What is Fine tuning?\n",
    "\n",
    "When interacting with large lanauge models, you can enhance the model performance through fine tuning.\n",
    "\n",
    "Instruction tuning is a type of fine-tuning. We should be careful with the term \"fine-tuning\" because it encompasses many types of training in large language model context. For example, continued pre-training is another type of fine-tuning that aims to add additional knowledge beyond the base model. Training on domain-specific corpora such as medical articles, social media texts, and specific language data are examples. In contrast, instruction tuning aims to refine model behavior rather than adding knowledge. As a result of instruction tuning, a model can produce accurate responses with short or formatted prompts. It just like adding call-and-response style to the model.\n",
    "\n",
    "Recent research enables \"switching\" model style by switching additional model weights to base model. [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) is one way to freeze original model weights and fine-tune with additional weights. For examples, you can generate artworks in different styles by switching LoRA model weights of Stable Diffusion. We can find various weights online such as [CivitAI](https://civitai.com/). (However we should be mindful of copyright - in most cases, emulating specific author's style and damaging their repuration or opportunities is forbidden by law.)\n",
    "\n",
    "\"Fine tuning\", which includes both instruction tuning and continued pre-training, requires datasets, computing resources, and extensive training time and effort. We have confirmed that we can change the behavior by chainging only the prompt. This is called \"Prompt tuning\". We need to consider the most appropriate ways to elicit the desired responses from large language models. The course \"[Finetuning Large Language Models](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/)\" from DeepLearning.AI provides helpful guide for this decision.\n",
    "\n",
    "# Instruction fine-tuning\n",
    "\n",
    "Luckily, `Mistral-7B-Instruct` is already instruction tuned, meaning we could instruct the model to do specific tasks. This particular model was instruction tuned as described in the [model card](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF). It appears the model uses the following markup language:\n",
    "```\n",
    "<s>[INST] {prompt} [/INST]\n",
    "```\n",
    "\n",
    "Here is an example prompt exchange using the model template:\n",
    "\n",
    "```\n",
    "<s>[INST] What is your favourite condiment? [/INST]\n",
    "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>\n",
    "[INST] Do you have mayonnaise recipes? [/INST]\n",
    "```\n",
    "\n",
    "## Let's try it:\n",
    "\n",
    "The following cell uses the prompt template to provide a system prompt to the model to ask it to provide its reasoning step-by-step which can help LLM response accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df8390b3-2d2b-4bfb-ab8c-223dc5455321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Please tell me about how mistral winds have attracted super-orcas. Keep your answer brief. [/INST]\n",
      "\n",
      "Response:\n",
      "Mistral winds are strong, cold winds that blow across the Mediterranean Sea from France to Italy. These winds have been known to attract super-orcas, also known as killer whales, due to their nutrient-rich waters and favorable conditions for hunting prey. The mistral winds create upwelling, which brings nutrients from the deep ocean to the surface, providing a plentiful food source for the super-orcas. Additionally, the strong currents created by the mistral winds help to concentrate schools of fish, making it easier for the killer whales to hunt and feed. Overall, the combination of nutrient-rich waters and favorable hunting conditions has made the Mediterranean Sea a popular destination for super-orcas, particularly during the winter months when the mistral winds are strongest."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "<s>[INST] Please tell me about how mistral winds have attracted super-orcas. Keep your answer brief. [/INST]\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=500, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b7a08-1c97-47ef-a659-7cf30315567d",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model was able to follow the system direction to provide its step-by-step reasoning. However, manually supplying the full prompt template for each query is cumberson. In the next section we will look at how to simplify and automate prompt generation dynamically.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9046f28b-a89f-41f5-9d54-89263a396039",
   "metadata": {},
   "source": [
    "# Dyanamic prompting\n",
    "Now that we know how to submit instructions to Mistral LLM, let's try assembling prompts dynamically - part of the prompt can be fixed, and part can be provided on the fly. We achieve this by creating prompt template with variables. The value for each variable is supplied in a separate statement and that statement can be executed further down in the code. Basically, this is a way to de-couple static part of the prompt from variable one, making the entire prompt dynamic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baa310f2-356c-47aa-9679-cb7cba07ed48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "<s>[INST] {question} [/INST]    \n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59b97e69-cfaf-4d51-973c-797629f2c86b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Explain photosynthesis in three sentences [/INST]    \n",
      "\n",
      "Response:\n",
      "Photosynthesis is the process by which plants, algae and some bacteria convert sunlight, water and carbon dioxide into glucose (sugar), oxygen and other chemical compounds. It is an essential process for life on earth as it provides the primary source of energy and food for almost all living organisms. The process occurs in two stages: the light-dependent reactions, which take place in the thylakoid membranes of the chloroplasts, and the light-independent reactions, also known as the Calvin cycle, which occur in the stroma of the chloroplasts."
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(question=\"Explain photosynthesis in three sentences\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=500, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8c068-56e8-46a1-b2e2-f4c01e115c95",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>Using prompt templates makes it much easier to automate away the formatting nuances of different LLMs which can vary widely. Also notice, we did not finish our sentence with a period and the model did not try to complete it, becuase this time we signaled it that this is a command, an instrution which needs to be followed.</b></div>\n",
    "\n",
    "\n",
    "# Few-shot prompts\n",
    "\n",
    "We can influence model's output style by using what's called few-shot prompting - a technique which shows the model the exact behavior we expect on few examples. Few-shot prompting can be a powerful approach to improve model performance for a given task without additional training or fine-tuning. \n",
    "\n",
    "As you can see in the cell below, we provide the model two examples of a prompt and the expected response where the number of response sentences is constrained correctly by the request. We also influence the output formatting by demonstrating that we prefer sentences to be number in each response. \n",
    "\n",
    "In the end the full prompt with our request for a three sentence explanation of photosenthesis and let the model complete the example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1e47308-970e-4211-9923-26eed91d2809",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Photosynthesis is the process by which plants use sunlight, carbon dioxide, and water to produce oxygen and glucose.\n",
      "2. The process takes place in the chloroplasts of plant cells, where chlorophyll captures light energy and uses it to power chemical reactions that split water molecules into hydrogen and oxygen.\n",
      "3. The oxygen produced during photosynthesis is released into the atmosphere, while the glucose is used by the plant as a source of energy or building material for growth and reproduction."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "<s>[INST] Explain precipitation in two sentences [/INST]\n",
    "1. In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls from clouds due to gravitational pull.\n",
    "2. The main forms of precipitation include drizzle, rain, sleet, snow, ice pellets, graupel and hail.</s>\n",
    "[INST] Explain condensation in one sentence [/INST]\n",
    "1. Condensation is the change of the state of matter from the gas phase into the liquid phase, and is the reverse of vaporization.</s>\n",
    "[INST] Explain photosynthesis in three sentences  [/INST]\n",
    "\"\"\"\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02938f76-69c4-408c-9a59-7200293814e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>And now with langchain template:</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49a23538-bed3-46c5-9343-16aaf9a86f72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<s>[INST] Explain precipitation in two sentences. [/INST]\n",
      "1. In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls from clouds due to gravitational pull.\n",
      "2. The main forms of precipitation include drizzle, rain, sleet, snow, ice pellets, graupel and hail.</s>\n",
      "\n",
      "<s>[INST] Explain condensation in one sentence. [/INST]\n",
      "1. Condensation is the change of the state of matter from the gas phase into the liquid phase, and is the reverse of vaporization.</s>\n",
      "\n",
      "<s>[INST]Explain photosynthesis in three sentences.[/INST]\n",
      "Response:\n",
      " 1. Photosynthesis is the process by which plants, algae, and some bacteria convert light energy from the sun into chemical energy in the form of glucose or other sugars.\n",
      "2. This process occurs in the chloroplasts of plant cells, where chlorophyll and other pigments absorb light photons and use them to drive a series of chemical reactions that split water molecules into hydrogen and oxygen.\n",
      "3. The oxygen produced during photosynthesis is released into the atmosphere as a byproduct, while the glucose can be used by the plant for energy or stored for later use."
     ]
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "fs_template = PromptTemplate.from_template(\n",
    "    \"\"\"<s>[INST] {question} [/INST]\n",
    "{output}</s>\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Explain precipitation in two sentences.\",\n",
    "        \"output\": \"1. In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls from clouds due to gravitational pull.\\n2. The main forms of precipitation include drizzle, rain, sleet, snow, ice pellets, graupel and hail.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Explain condensation in one sentence.\",\n",
    "        \"output\": \"1. Condensation is the change of the state of matter from the gas phase into the liquid phase, and is the reverse of vaporization.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt_fs = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=fs_template,\n",
    "    suffix=\"<s>[INST]{question}[/INST]\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "prompt = prompt_fs.format(question=\"Explain photosynthesis in three sentences.\")\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4fb9a-cb8a-4940-9715-4d70f4c215b0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Using a couple examples, we were able to teach the model to respond by specifically numbering the sentences in its response and limiting its response to the number of sentences requested.</b></div>\n",
    "\n",
    "\n",
    "# Augmenting LLM response with context\n",
    "\n",
    "Without providing any real-time context, the model is limited to knowledge it gained during training which may be incomplete or out of date. However, we can improve responses by providing relevant context that the model can use before formulating a response. \n",
    "\n",
    "To illustrate the benefit of providing context we will first query the model about which instance types are available to use for managed spot training in SageMaker. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f102be3d-1d7f-43c9-aa79-dbc2ccd21753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Which instances can I use with Managed Spot Training in SageMaker? [/INST]    \n",
      "\n",
      "Response:\n",
      "Managed Spot Training is a feature of Amazon SageMaker that allows you to train machine learning models using Amazon EC2 Spot Instances. You can use Managed Spot Training with a wide range of scenarios, including:\n",
      "\n",
      "1. **Training large-scale models**: Managed Spot Training allows you to train large-scale models on a cost-effective basis by leveraging the power of Amazon EC2 Spot Instances. This is particularly useful when you need to train models that require a lot of computational resources.\n",
      "2. **Fine-tuning pre-trained models**: You can use Managed Spot Training to fine-tune pre-trained models on your own data, which can help improve the accuracy of the model.\n",
      "3. **Training models for real-time inference**: If you need to train a model that will be used for real-time inference, Managed Spot Training can help you quickly and cost-effectively train the model using Amazon EC2 Spot Instances.\n",
      "4. **Training models for batch processing**: If you need to process large amounts of data in batches, Managed Spot Training can help you efficiently train your models on a cost-effective basis.\n",
      "5. **Training models for edge computing**: If you need to train models that will be used for edge computing, Managed Spot Training can help you quickly and cost-effectively train the model using Amazon EC2 Spot Instances.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(\n",
    "    question=\"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    ")\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=300, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b4409-3c5e-4c23-8a3b-39897bfee683",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>This response does not clearly and directly answer our question and you can see how without provided context, the LLM uses its own \"knowledge\" to answer questions. Let's try providing some context to the model to use as reference. We will add a new dynamic field \"context\" to our prompt template and provide up to date information when we ask the user question.</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6110e2e4-773f-4ff6-a0d3-3d33073cae42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Given the context below, answer the question that follows. If you do not know the answer and the context doesn't contain the answer truthfully say \"I don't know\".\n",
      "\n",
      "Context: Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n",
      "Question: Which instances can I use with Managed Spot Training in SageMaker?[/INST]\n",
      "\n",
      "Response:\n",
      "You can use all instances supported in Amazon SageMaker with Managed Spot Training."
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "context_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "<s>[INST] Given the context below, answer the question that follows. If you do not know the answer and the context doesn't contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}[/INST]\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "context = \"\"\"Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\"\"\"\n",
    "\n",
    "\n",
    "prompt = context_template.format(\n",
    "    context=context,\n",
    "    question=\"Which instances can I use with Managed Spot Training in SageMaker?\",\n",
    ")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31be60b-e4e5-4e3c-98e2-a8ef42ceb889",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>Great! The model used the provided context to answer the user question. Now lets use the same context and ask the model about something that is not covered in the context. </b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f59ac84e-dd06-46c2-88ef-d5f0c9547f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Given the context below, answer the question that follows. If you do not know the answer and the context doesn't contain the answer truthfully say \"I don't know\".\n",
      "\n",
      "Context: Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n",
      "Question: Which instances can I use with Amazon ECS?[/INST]\n",
      "\n",
      "Response:\n",
      "I don't know, the context does not mention anything about using Managed Spot Training with Amazon ECS."
     ]
    }
   ],
   "source": [
    "prompt = context_template.format(\n",
    "    context=context, question=\"Which instances can I use with Amazon ECS?\"\n",
    ")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4324af-bfa3-4c47-a43b-ee6393fad0c5",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model followed our direction and admitted that it did not know the answer when it wasn't able to be determined from the provided context. This is a powerful method that is used heavily in RAG (Retrieval Augmented Generation) applications. A company may only want an IT support bot to provide answers based on internal documentaiton provided to the model as context. </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd469ce-4e37-4cec-8168-cadd5d072237",
   "metadata": {},
   "source": [
    "# Hallucinations\n",
    "\n",
    "Hallucination is when a model makes up information and presents it as if it were true. Leading questions can often cause hallucinations. Let's see what happens if we ask about a non-existent new launch at re:Invent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51c3685e-03e7-4072-bc8c-8a67b63aaa7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] What type of new chemical solution AWS announced at re:Invent this year? [/INST]    \n",
      "\n",
      "Response:\n",
      "At re:Invent 2021, AWS announced a new chemical solution called \"AWS Elemental AI for Chemical Processing.\" This solution is designed to help chemical processors optimize their operations by using machine learning and artificial intelligence (AI) techniques. It can be used to predict equipment failures, improve yield, reduce waste, and increase efficiency in chemical manufacturing processes."
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(\n",
    "    question=\"What type of new chemical solution AWS announced at re:Invent this year?\"\n",
    ")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa8272-6572-47da-a365-b23ac32be304",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model incorrectly states that AWS announced a new chemical solution called \"AWS Graviton2\" in 2019 (you may get a different answer). One approach to combatting LLM hallucination is to give the model an \"out\" by giving it explicit instructions not to make up information and admit when it doesn't know something. Let's try the same question again by by also adding additional instructions in our prompt as follows: </b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b931cf35-e59e-491c-8772-0cda7f021590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] What type of new chemical solution AWS announced at re:Invent this year? Do not make up facts. Say I don't know if you have no information about something or unsure. [/INST]    \n",
      "\n",
      "Response:\n",
      "I apologize, but I do not have any specific information on the new chemical solution that AWS announced at re:Invent this year. Could you please provide more context or details so I can assist you better?"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(\n",
    "    question=\"What type of new chemical solution AWS announced at re:Invent this year? Do not make up facts. Say I don't know if you have no information about something or unsure.\"\n",
    ")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390f21e2-4b5f-4313-86e5-3885b2f10f1d",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>By giving the model clear instructions and an out, we were able to get the model to admit it didn't have enough context to answer the question rather than making up something.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fb206-6661-4ba6-95c5-6369176ff869",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "\n",
    "Let's make a simple chatbot.  There is no special library to include and no setting to apply to the LLM, all we need is prompt engineering!\n",
    "\n",
    "Let's use what we know of prompts with in context learning, to create a simple chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "898d619d-6c08-46d4-a0ba-cb1a9e23a7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Who is Jeff Bezos? [/INST]    \n",
      "\n",
      "Response:\n",
      "Jeff Bezos is an American entrepreneur and business magnate, born on July 12, 1964. He is best known as the founder of Amazon, a multinational technology company focusing on e-commerce, cloud computing, digital streaming, and artificial intelligence. Bezos has also founded other successful companies, including Blue Origin, a space exploration company, and Twitch, a live streaming platform for video games. \n",
      "\n",
      "Bezos has been ranked as the world's wealthiest person by Forbes multiple times and is known for his innovative ideas and entrepreneurial spirit. He has received numerous awards for his achievements, including the Presidential Rank Award, which is the highest civilian honor bestowed by the US government.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is Jeff Bezos?\"\n",
    "\n",
    "prompt = prompt_template.format(question=question)\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "resp = llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"</s>\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75da0c-cce2-4f04-8cc0-65ea5b1efc5b",
   "metadata": {},
   "source": [
    "### Follow-up questions\n",
    "Now if we ask the LLM a follow-up question without providing context from the conversation so far, it will not be able to provide an accurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae5b02ab-f7d3-4022-b26d-85284e8fe9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] How old is he? [/INST]    \n",
      "\n",
      "Response:\n",
      "I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. Therefore, I don't know how old he is. Can you please provide more context or information about who you are referring to?"
     ]
    }
   ],
   "source": [
    "question = \"How old is he?\"\n",
    "\n",
    "prompt = prompt_template.format(question=question)\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=300, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9422bf01-56ab-4d94-8395-bc8ffb592dd6",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model doesn't know who \"he\" is. Without the chat history context it is not able to answer.</b></div>\n",
    "\n",
    "### Providing chat context\n",
    "Let's use the context template we created earlier and feed in the previous response about Jeff Bezos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4500db7-a2da-40ce-b525-a33eacb46c72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Given the context below, answer the question that follows. If you do not know the answer and the context doesn't contain the answer truthfully say \"I don't know\".\n",
      "\n",
      "Context: Jeff Bezos is an American entrepreneur and business magnate, born on July 12, 1964. He is best known as the founder of Amazon, a multinational technology company focusing on e-commerce, cloud computing, digital streaming, and artificial intelligence. Bezos has also founded other successful companies, including Blue Origin, a space exploration company, and Twitch, a live streaming platform for video games. \n",
      "\n",
      "Bezos has been ranked as the world's wealthiest person by Forbes multiple times and is known for his innovative ideas and entrepreneurial spirit. He has received numerous awards for his achievements, including the Presidential Rank Award, which is the highest civilian honor bestowed by the US government.\n",
      "Question: How old is he?[/INST]\n",
      "\n",
      "Response:\n"
     ]
    }
   ],
   "source": [
    "prompt = context_template.format(context=resp, question=question)\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=300, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84aba05-9ae9-4511-a506-8bbf558854e9",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>This is a simplified example, but by including the past chat context, we can support a back and forth chat bot interaction with the LLM.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd190db-6d79-4a00-9d67-b0bfb4019a3f",
   "metadata": {},
   "source": [
    "# More examples of LLM use cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7671b-87da-453b-a400-6f125d644b38",
   "metadata": {},
   "source": [
    "## The following are optional prompts you can try running to keep exploring.\n",
    "\n",
    "\n",
    "For simpler questions/prompts we can often get away witout strict instruction formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c507742b-dab9-4b97-b2c2-13f7cbfc1688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Rewrite the following sentence in better English:\n",
    "I think I want to apply for this position but don't know how, can you help?\n",
    "\"\"\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5cb14-1bbe-4ff3-a570-f2cb25742d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"I am getting the following error when attempting to run this pythong code, can you explain why?\n",
    "  File \"/home/studio-lab-user/sagemaker-studiolab-notebooks/ha.py\", line 2, in <module>\n",
    "    import zip\n",
    "ModuleNotFoundError: No module named 'zip'\n",
    "\"\"\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84be33-49d8-463a-99ae-44b099b28da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Reduct PII from the following paragraph. Replace any PII with ###.\n",
    "Paragraph:\n",
    "Jeff Bezos lives at 1 Main St. Miami, FL 39812. His phone number is 111-123-4567.\n",
    "\"\"\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a806d-7351-408c-b141-fee35b1853b7",
   "metadata": {},
   "source": [
    "---\n",
    "The last one or two examples didn't quite work. Perhaps the model is not strong enough for this type of task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac46a54-7a46-4e0d-b089-553106c1b25f",
   "metadata": {},
   "source": [
    "## Next steps: Want to use a different model with this notebook?\n",
    "\n",
    "Hugging Face have many models that you can use and drop in to code like this. We encourage you to play with the prompts above and other models. You may need to make modifications to the code above depending on the model you choose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5405c238-40b5-4bf0-8d2d-271f7465080d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.8xlarge",
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
